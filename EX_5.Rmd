---
title: "Exercise 5"
subtitle: "for Advanced Methods for Regression and Classification"
author: "Dzhamilia Kulikieva"
date: 20.11.2024
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, echo=TRUE}
library(ROCit)

data(Loan)

# Convert Status to a numeric variable (0 and 1)
Loan$Status <- as.numeric(Loan$Status) - 1
str(Loan)
head(Loan)


```
# 1. Building linear regression model for the binary variable Status

```{r, echo=TRUE}
# Split the data
set.seed(1234)
train_indices <- sample(1:nrow(Loan), size = 2/3*nrow(Loan))
train_data <- Loan[train_indices, ]
test_data <- Loan[-train_indices, ]

# Fit a linear regression model on the training data
lm_model <- lm(Status ~., data = train_data)
summary(lm_model)

```

# 2. Inspecting the outcome of summary()

"Term" and "Score" have NA values for both coefficients and standard errors. This indicates that these variables are singular or highly collinear with other variables, meaning they do not provide unique information for the model.

Predictors like EmpLenB, EmpLenC, EmpLenD, EmpLenU, and HomeOWN  have high p-values, meaning they do not significantly contribute to the model.

The model has a low R-squared value of 0.07046, meaning that only about 7% of the variance in the target variable Status is explained by the model. This suggests that the model is not performing well and could be improved.

Predictores with small p-value like Amount, Income, and ILR show whether the variables are significant for the model. Although they may have very different scales, that can negatively impact the performance of some algorithms.

# 3. Inspecting the plot() of LM

```{r, echo=TRUE}
plot(lm_model)

```

In each plot we see two parallel lines of points (one below zero), that indicates an issue with using linear regression for a binary target variable. Linear regression assumes a continuous dependent variable. For binary classification tasks, LR tries to fit the data as if the target were continuous, resulting in residuals clustering into two distinct parallel lines(Residuals vs Fitted) around values close to 0 and 1.

The patterns in the other diagnostic plots also all point to the same conclusion: LR is not the right tool for a binary target variable like Status. The model fails to meet critical assumptions (normality, homoscedasticity, and linearity), and the unusual plot shapes are direct consequences of trying to apply a continuous model to categorical data.

# 4. Prediction of the responce value for the train_data

```{r, echo=TRUE}

train_predictions <- predict(lm_model, newdata = train_data)
plot(train_data$Status, train_predictions, main = "Predictions vs Observations", xlab = "Observations", ylab = "Predicted Values")

```

Status is a binary variable that takes two values (0 and 1). These values are represented as categories, so all points with the same Status values line up in vertical lines on the graph.

For cutoff values, I would try setting a threshold (e.g., 0.5) and see how it affects the predictions.

# 5. Confusion Matrix for the Training Set.

```{r, echo=TRUE}

# Apply cutoff (e.g., 0.5)
predicted_class <- ifelse(train_predictions > 0.5, 1, 0)

# Confusion matrix
table(Actual = train_data$Status, Predicted = predicted_class)

```
Actual 0, Predicted 1 (86):
FP: The model incorrectly predicted 86 instances as belonging to class 1 when they actually belong to class 0.

Actual 1, Predicted 1 (514):
TP: The model correctly predicted 514 instances as belonging to class 1 when they truly belong to class 1.

# 6. Evaluating classifier and plotting the result


```{r, echo=TRUE}

roc_result <- rocit(score = train_predictions, class = train_data$Status)
summary(roc_result)
plot(roc_result)

```

Area Under the Curve (AUC) value is a key indicator of classifier quality. AUC = 0.7061 suggests that the model has moderate performance. It’s better than random guessing (since it's above 0.5), but it is not performing excellently.

ROC curve on the graph is above the diagonal line, meaning the model is performing better than random guessing.
With x ~ 0.2 and y~ 0.5 is the point that is farthest from the diagonal and corresponds to the highest value of the Youden Index on the ROC curve, then this is the optimal cutoff for classification.


```{r, echo=TRUE}
# Initialize vectors
cutoff_values <- seq(0, 1, by = 0.05)
balanced_accuracies <- numeric(length(cutoff_values))

# Loop through cutoff values and calculate balanced accuracy
for (i in seq_along(cutoff_values)) {
  predicted_class <- ifelse(train_predictions > cutoff_values[i], 1, 0)
  measure_result <- measureit(score = predicted_class, class = train_data$Status, measure = c("TPR", "TNR"))
  balanced_accuracies[i] <- mean(c(measure_result$TPR, measure_result$TNR))
}

# Plot balanced accuracy vs cutoff
plot(cutoff_values, balanced_accuracies, type = "b", 
     xlab = "Cutoff Value", ylab = "Balanced Accuracy", 
     main = "Balanced Accuracy vs Cutoff Value")

# Find optimal cutoff (max balanced accuracy)
optimal_cutoff <- cutoff_values[which.max(balanced_accuracies)]
abline(v = optimal_cutoff, col = "red", lty = 2)  # Add vertical line at optimal cutoff

# Print optimal cutoff and balanced accuracy
cat("Optimal cutoff:", optimal_cutoff, "\n")
cat("Balanced accuracy at optimal cutoff:", max(balanced_accuracies), "\n")


```


```{r, echo=TRUE}
# Predict on the test set using the optimal cutoff
test_predictions_prob <- predict(lm_model, newdata = test_data, type = "response")
test_predicted_class <- ifelse(test_predictions_prob > optimal_cutoff, 1, 0)

# Confusion matrix for test set
test_confusion_matrix <- table(Actual = test_data$Status, Predicted = test_predicted_class)
print(test_confusion_matrix)


```

True Negatives (TN):
29 cases — the true class is 0, and the predicted class is also 0.
These are correct predictions.

False Positives (FP):
16 cases — the true class is 0, but the model predicted 1.
These are Type I errors (false alarms).

False Negatives (FN):
122 cases — the true class is 1, but the model predicted 0.
These are Type II errors (missed detections).

True Positives (TP):
133 cases — the true class is 1, and the model correctly predicted 1.
These are correct predictions.








