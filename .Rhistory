balanced_accuracy <- (TPR + TNR) / 2
list(
MisclassificationRate = misclassification_rate,
BalancedAccuracy = balanced_accuracy
)
install.packages('dplyr')
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
# Deleting IntRate
train_data <- train_data %>% select(-IntRate)
library(MASS)
lda_model <- lda(Status ~., data = train_data)
install.packages('MASS')
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
lda_model <- lda(Status ~., data = train_data)
# Predict on training data
train_predictions <- predict(lda_model, train_data)$class
# Confusion matrix
conf_matrix_train <- table(Predicted = train_predictions, Actual = train_data$Status)
# Compute evaluation metrics
TP <- conf_matrix_train[2, 2]
TN <- conf_matrix_train[1, 1]
FP <- conf_matrix_train[2, 1]
FN <- conf_matrix_train[1, 2]
misclassification_rate <- (FP + FN) / sum(conf_matrix_train)
TPR <- TP / (TP + FN) # Sensitivity
TNR <- TN / (TN + FP) # Specificity
balanced_accuracy <- (TPR + TNR) / 2
list(
MisclassificationRate = misclassification_rate,
BalancedAccuracy = balanced_accuracy
)
library(MASS)
lda_model <- lda(Status ~., data = train_data)
summary(lda_model)
print(conf_matrix_train)
# Predict on training data
train_predictions <- predict(lda_model, train_data)$class
# Confusion matrix
conf_matrix_train <- table(Predicted = train_predictions, Actual = train_data$Status)
print(conf_matrix_train)
# Compute evaluation metrics
TP <- conf_matrix_train[2, 2]
TN <- conf_matrix_train[1, 1]
FP <- conf_matrix_train[2, 1]
FN <- conf_matrix_train[1, 2]
misclassification_rate <- (FP + FN) / sum(conf_matrix_train)
TPR <- TP / (TP + FN) # Sensitivity
TNR <- TN / (TN + FP) # Specificity
balanced_accuracy <- (TPR + TNR) / 2
list(
MisclassificationRate = misclassification_rate,
BalancedAccuracy = balanced_accuracy
)
# Deleting IntRate
test_data <- test_data %>% select(-IntRate)
library(dplyr)
# Deleting IntRate
test_data <- test_data %>% select(-IntRate)
# Predict on test data
test_predictions <- predict(lda_model, test_data)$class
# Confusion matrix for test data
conf_matrix_test <- table(Predicted = test_predictions, Actual = test_data$Status)
# Compute evaluation metrics
TP <- conf_matrix_test[2, 2]
TN <- conf_matrix_test[1, 1]
FP <- conf_matrix_test[2, 1]
FN <- conf_matrix_test[1, 2]
misclassification_rate_test <- (FP + FN) / sum(conf_matrix_test)
TPR <- TP / (TP + FN) # Sensitivity
TNR <- TN / (TN + FP) # Specificity
balanced_accuracy_test <- (TPR + TNR) / 2
list(
MisclassificationRate = misclassification_rate_test,
BalancedAccuracy = balanced_accuracy_test
)
library(dplyr)
# Deleting IntRate
test_data <- test_data %>% select(-IntRate)
# Predict on test data
test_predictions <- predict(lda_model, test_data)$class
# Confusion matrix for test data
conf_matrix_test <- table(Predicted = test_predictions, Actual = test_data$Status)
print(conf_matrix_test)
# Compute evaluation metrics
TP <- conf_matrix_test[2, 2]
TN <- conf_matrix_test[1, 1]
FP <- conf_matrix_test[2, 1]
FN <- conf_matrix_test[1, 2]
misclassification_rate_test <- (FP + FN) / sum(conf_matrix_test)
TPR <- TP / (TP + FN) # Sensitivity
TNR <- TN / (TN + FP) # Specificity
balanced_accuracy_test <- (TPR + TNR) / 2
list(
MisclassificationRate = misclassification_rate_test,
BalancedAccuracy = balanced_accuracy_test
)
knitr::opts_chunk$set(echo = TRUE)
library(ROCit)
data(Loan)
# Convert Status to a numeric variable (0 and 1)
Loan$Status <- as.numeric(Loan$Status) - 1
Loan$EmpLen <- as.numeric(factor(Loan$EmpLen))  # Преобразуем в числовые уровни
Loan$Home <- as.numeric(factor(Loan$Home))
str(Loan)
Loan$Term <- NULL
normalize <- function(x) {
return((x-min(x)) / (max(x) - min(x)))
}
Loan$Amount <- normalize(Loan$Amount)
Loan$IntRate <- normalize(Loan$IntRate)
Loan$ILR <- normalize(Loan$ILR)
Loan$Income <- normalize(Loan$Income)
Loan$Score <- normalize(Loan$Score)
# Split the data
set.seed(1234)
train_indices <- sample(1:nrow(Loan), size = 2/3*nrow(Loan))
train_data <- Loan[train_indices, ]
test_data <- Loan[-train_indices, ]
library(MASS)
lda_model <- lda(Status ~., data = train_data)
summary(lda_model)
cor(train_data[, -which(names(train_data) == "Status")])
library(dplyr)
# Deleting IntRate
train_data <- train_data %>% select(-IntRate)
library(MASS)
lda_model <- lda(Status ~., data = train_data)
summary(lda_model)
# Predict on training data
train_predictions <- predict(lda_model, train_data)$class
# Confusion matrix
conf_matrix_train <- table(Predicted = train_predictions, Actual = train_data$Status)
print(conf_matrix_train)
# Compute evaluation metrics
TP <- conf_matrix_train[2, 2]
TN <- conf_matrix_train[1, 1]
FP <- conf_matrix_train[2, 1]
FN <- conf_matrix_train[1, 2]
misclassification_rate <- (FP + FN) / sum(conf_matrix_train)
TPR <- TP / (TP + FN) # Sensitivity
TNR <- TN / (TN + FP) # Specificity
balanced_accuracy <- (TPR + TNR) / 2
list(
MisclassificationRate = misclassification_rate,
BalancedAccuracy = balanced_accuracy
)
# Predict on test data
test_predictions <- predict(lda_model, test_data)$class
# Confusion matrix for test data
conf_matrix_test <- table(Predicted = test_predictions, Actual = test_data$Status)
print(conf_matrix_test)
# Compute evaluation metrics
TP <- conf_matrix_test[2, 2]
TN <- conf_matrix_test[1, 1]
FP <- conf_matrix_test[2, 1]
FN <- conf_matrix_test[1, 2]
misclassification_rate_test <- (FP + FN) / sum(conf_matrix_test)
TPR <- TP / (TP + FN) # Sensitivity
TNR <- TN / (TN + FP) # Specificity
balanced_accuracy_test <- (TPR + TNR) / 2
list(
MisclassificationRate = misclassification_rate_test,
BalancedAccuracy = balanced_accuracy_test
)
n1 <- sum(train_data$Status == 0)
n2 <- sum(train_data$Status == 1)
min_n <- min(n1, n2)
n1 <- sum(train_data$Status == 0)
n2 <- sum(train_data$Status == 1)
min_n <- min(n1, n2)
min_n
n1 <- sum(train_data$Status == 0)
n2 <- sum(train_data$Status == 1)
min_n <- min(n1, n2)
set.seed(123) # For reproducibility
group_0 <- train_data[train_data$Status == 0, ]
group_1 <- train_data[train_data$Status == 1, ]
undersampled_0 <- group_0[sample(nrow(group_0), min_n), ]
undersampled_1 <- group_1[sample(nrow(group_1), min_n), ]
balanced_train <- rbind(undersampled_0, undersampled_1)
library(MASS)
lda_model <- lda(Status ~ ., data = balanced_train)
# Predictions on training set
train_pred <- predict(lda_model, newdata = balanced_train)$class
# Predictions on test set
test_pred <- predict(lda_model, newdata = test_data)$class
# Calculate metrics
library(caret)
install.packages('caret')
library(MASS)
lda_model <- lda(Status ~ ., data = balanced_train)
# Predictions on training set
train_pred <- predict(lda_model, newdata = balanced_train)$class
# Predictions on test set
test_pred <- predict(lda_model, newdata = test_data)$class
# Calculate metrics
library(caret)
confusion_train <- confusionMatrix(as.factor(train_pred), as.factor(balanced_train$Status))
confusion_test <- confusionMatrix(as.factor(test_pred), as.factor(test_data$Status))
list(
TrainMisclassificationRate = 1 - confusion_train$overall['Accuracy'],
TrainBalancedAccuracy = confusion_train$byClass['Balanced Accuracy'],
TestMisclassificationRate = 1 - confusion_test$overall['Accuracy'],
TestBalancedAccuracy = confusion_test$byClass['Balanced Accuracy']
)
library(MASS)
lda_model <- lda(Status ~ ., data = balanced_train)
# Predictions on training set
train_pred <- predict(lda_model, newdata = balanced_train)$class
# Predictions on test set
test_pred <- predict(lda_model, newdata = test_data)$class
# Calculate metrics
# Confusion matrix
conf_train <- table(Predicted = train_pred, Actual = balanced_train$Status)
print(conf_train)
# Compute evaluation metrics
TP <- conf_train[2, 2]
TN <- conf_train[1, 1]
FP <- conf_train[2, 1]
FN <- conf_train[1, 2]
Train_misclassification_rate <- (FP + FN) / sum(conf_matrix_train)
TPR <- TP / (TP + FN) # Sensitivity
TNR <- TN / (TN + FP) # Specificity
Train_balanced_accuracy <- (TPR + TNR) / 2
list(
MisclassificationRate = Train_misclassification_rate,
BalancedAccuracy = Train_balanced_accuracy
)
# Confusion matrix for test data
conf_test <- table(Predicted = test_pred, Actual = test_data$Status)
print(conf_test)
# Compute evaluation metrics
TP <- conf_test[2, 2] # True Positives
TN <- conf_test[1, 1] # True Negatives
FP <- conf_test[2, 1] # False Positives
FN <- conf_test[1, 2] # False Negatives
misclassification_rate_test <- (FP + FN) / sum(conf_matrix_test) # Misclassification Rate
TPR <- TP / (TP + FN)  # Sensitivity or True Positive Rate
TNR <- TN / (TN + FP)  # Specificity or True Negative Rate
balanced_accuracy_test <- (TPR + TNR) / 2 # Balanced Accuracy
# Print the results
list(
MisclassificationRate = misclassification_rate_test,
BalancedAccuracy = balanced_accuracy_test
)
library(MASS)
lda_model <- lda(Status ~ ., data = balanced_train)
# Predictions on training set
train_pred <- predict(lda_model, newdata = balanced_train)$class
# Predictions on test set
test_pred <- predict(lda_model, newdata = test_data)$class
# Calculate metrics
# Confusion matrix
conf_train <- table(Predicted = train_pred, Actual = balanced_train$Status)
print(conf_train)
# Compute evaluation metrics
TP <- conf_train[2, 2]
TN <- conf_train[1, 1]
FP <- conf_train[2, 1]
FN <- conf_train[1, 2]
Train_misclassification_rate <- (FP + FN) / sum(conf_train)
TPR <- TP / (TP + FN) # Sensitivity
TNR <- TN / (TN + FP) # Specificity
Train_balanced_accuracy <- (TPR + TNR) / 2
list(
MisclassificationRate = Train_misclassification_rate,
BalancedAccuracy = Train_balanced_accuracy
)
# Confusion matrix for test data
conf_test <- table(Predicted = test_pred, Actual = test_data$Status)
print(conf_test)
# Compute evaluation metrics
TP <- conf_test[2, 2] # True Positives
TN <- conf_test[1, 1] # True Negatives
FP <- conf_test[2, 1] # False Positives
FN <- conf_test[1, 2] # False Negatives
misclassification_rate_test <- (FP + FN) / sum(conf_test) # Misclassification Rate
TPR <- TP / (TP + FN)  # Sensitivity or True Positive Rate
TNR <- TN / (TN + FP)  # Specificity or True Negative Rate
balanced_accuracy_test <- (TPR + TNR) / 2 # Balanced Accuracy
# Print the results
list(
MisclassificationRate = misclassification_rate_test,
BalancedAccuracy = balanced_accuracy_test
)
# Confusion matrix for test data
conf_test <- table(Predicted = test_pred, Actual = test_data$Status)
print(conf_test)
# Compute evaluation metrics
TP <- conf_test[2, 2] # True Positives
TN <- conf_test[1, 1] # True Negatives
FP <- conf_test[2, 1] # False Positives
FN <- conf_test[1, 2] # False Negatives
misclassification_rate_test <- (FP + FN) / sum(conf_test) # Misclassification Rate
TPR <- TP / (TP + FN)  # Sensitivity or True Positive Rate
TNR <- TN / (TN + FP)  # Specificity or True Negative Rate
balanced_accuracy_test <- (TPR + TNR) / 2 # Balanced Accuracy
# Print the results
list(
MisclassificationRate = misclassification_rate_test,
BalancedAccuracy = balanced_accuracy_test
)
# Predictions on test set
test_pred <- predict(lda_model, newdata = test_data)$class
# Confusion matrix for test data
conf_test <- table(Predicted = test_pred, Actual = test_data$Status)
print(conf_test)
# Compute evaluation metrics
TP <- conf_test[2, 2] # True Positives
TN <- conf_test[1, 1] # True Negatives
FP <- conf_test[2, 1] # False Positives
FN <- conf_test[1, 2] # False Negatives
misclassification_rate_test <- (FP + FN) / sum(conf_test) # Misclassification Rate
TPR <- TP / (TP + FN)  # Sensitivity or True Positive Rate
TNR <- TN / (TN + FP)  # Specificity or True Negative Rate
balanced_accuracy_test <- (TPR + TNR) / 2 # Balanced Accuracy
# Print the results
list(
MisclassificationRate = misclassification_rate_test,
BalancedAccuracy = balanced_accuracy_test
)
lda_model <- lda(Status ~ ., data = balanced_train2)
max_n <- max(n1, n2)
set.seed(123) # For reproducibility
oversampled_0 <- group_0[sample(nrow(group_0), max_n, replace = TRUE), ]
oversampled_1 <- group_1[sample(nrow(group_1), max_n, replace = TRUE), ]
balanced_train2 <- rbind(oversampled_0, oversampled_1)
lda_model <- lda(Status ~ ., data = balanced_train2)
# Predictions on training set
train_pred2 <- predict(lda_model, newdata = balanced_train2)$class
# Calculate metrics
# Confusion matrix
conf_train2 <- table(Predicted = train_pred2, Actual = balanced_train2$Status)
print(conf_train2)
# Compute evaluation metrics
TP <- conf_train2[2, 2]
TN <- conf_train2[1, 1]
FP <- conf_train2[2, 1]
FN <- conf_train2[1, 2]
Train_misclassification_rate <- (FP + FN) / sum(conf_train2)
TPR <- TP / (TP + FN) # Sensitivity
TNR <- TN / (TN + FP) # Specificity
Train_balanced_accuracy <- (TPR + TNR) / 2
list(
MisclassificationRate = Train_misclassification_rate,
BalancedAccuracy = Train_balanced_accuracy
)
# Predictions on test set
test_pred2 <- predict(lda_model, newdata = test_data)$class
# Confusion matrix for test data
conf_test2 <- table(Predicted = test_pred2, Actual = test_data$Status)
print(conf_test)
# Compute evaluation metrics
TP <- conf_test2[2, 2] # True Positives
TN <- conf_test2[1, 1] # True Negatives
FP <- conf_test2[2, 1] # False Positives
FN <- conf_test2[1, 2] # False Negatives
misclassification_rate_test <- (FP + FN) / sum(conf_test2) # Misclassification Rate
TPR <- TP / (TP + FN)  # Sensitivity or True Positive Rate
TNR <- TN / (TN + FP)  # Specificity or True Negative Rate
balanced_accuracy_test <- (TPR + TNR) / 2 # Balanced Accuracy
# Print the results
list(
MisclassificationRate = misclassification_rate_test,
BalancedAccuracy = balanced_accuracy_test
)
library(MASS)
lda_model_undersampled <- lda(Status ~ ., data = balanced_train)
# Predictions on training set
train_pred <- predict(lda_model_undersampled, newdata = balanced_train)$class
# Calculate metrics
# Confusion matrix
conf_train <- table(Predicted = train_pred, Actual = balanced_train$Status)
print(conf_train)
# Compute evaluation metrics
TP <- conf_train[2, 2]
TN <- conf_train[1, 1]
FP <- conf_train[2, 1]
FN <- conf_train[1, 2]
Train_misclassification_rate <- (FP + FN) / sum(conf_train)
TPR <- TP / (TP + FN) # Sensitivity
TNR <- TN / (TN + FP) # Specificity
Train_balanced_accuracy <- (TPR + TNR) / 2
list(
MisclassificationRate = Train_misclassification_rate,
BalancedAccuracy = Train_balanced_accuracy
)
# Predictions on test set
test_pred <- predict(lda_model_undersampled, newdata = test_data)$class
# Confusion matrix for test data
conf_test <- table(Predicted = test_pred, Actual = test_data$Status)
print(conf_test)
# Compute evaluation metrics
TP <- conf_test[2, 2] # True Positives
TN <- conf_test[1, 1] # True Negatives
FP <- conf_test[2, 1] # False Positives
FN <- conf_test[1, 2] # False Negatives
misclassification_rate_test <- (FP + FN) / sum(conf_test) # Misclassification Rate
TPR <- TP / (TP + FN)  # Sensitivity or True Positive Rate
TNR <- TN / (TN + FP)  # Specificity or True Negative Rate
balanced_accuracy_test <- (TPR + TNR) / 2 # Balanced Accuracy
# Print the results
list(
MisclassificationRate = misclassification_rate_test,
BalancedAccuracy = balanced_accuracy_test
)
lda_model_oversampled <- lda(Status ~ ., data = balanced_train2)
# Predictions on training set
train_pred2 <- predict(lda_model_oversampled, newdata = balanced_train2)$class
# Calculate metrics
# Confusion matrix
conf_train2 <- table(Predicted = train_pred2, Actual = balanced_train2$Status)
print(conf_train2)
# Compute evaluation metrics
TP <- conf_train2[2, 2]
TN <- conf_train2[1, 1]
FP <- conf_train2[2, 1]
FN <- conf_train2[1, 2]
Train_misclassification_rate <- (FP + FN) / sum(conf_train2)
TPR <- TP / (TP + FN) # Sensitivity
TNR <- TN / (TN + FP) # Specificity
Train_balanced_accuracy <- (TPR + TNR) / 2
list(
MisclassificationRate = Train_misclassification_rate,
BalancedAccuracy = Train_balanced_accuracy
)
# Predictions on test set
test_pred2 <- predict(lda_model_oversampled, newdata = test_data)$class
# Confusion matrix for test data
conf_test2 <- table(Predicted = test_pred2, Actual = test_data$Status)
print(conf_test)
# Compute evaluation metrics
TP <- conf_test2[2, 2] # True Positives
TN <- conf_test2[1, 1] # True Negatives
FP <- conf_test2[2, 1] # False Positives
FN <- conf_test2[1, 2] # False Negatives
misclassification_rate_test <- (FP + FN) / sum(conf_test2) # Misclassification Rate
TPR <- TP / (TP + FN)  # Sensitivity or True Positive Rate
TNR <- TN / (TN + FP)  # Specificity or True Negative Rate
balanced_accuracy_test <- (TPR + TNR) / 2 # Balanced Accuracy
# Print the results
list(
MisclassificationRate = misclassification_rate_test,
BalancedAccuracy = balanced_accuracy_test
)
# Train QDA model on undersampled data
qda_model_undersample <- qda(Status ~ ., data = balanced_train)
# Train QDA model on oversampled data
qda_model_oversample <- qda(Status ~ ., data = balanced_train2)
# Predictions on test set for both models
test_pred_undersample <- predict(qda_model_undersample, newdata = test_data)$class
test_pred_oversample <- predict(qda_model_oversample, newdata = test_data)$class
# Calculate confusion matrices for test set
conf_test_undersample <- table(Predicted = test_pred_undersample, Actual = test_data$Status)
conf_test_oversample <- table(Predicted = test_pred_oversample, Actual = test_data$Status)
# Compute evaluation metrics for both models
compute_metrics <- function(conf_matrix) {
TP <- conf_matrix[2, 2]
TN <- conf_matrix[1, 1]
FP <- conf_matrix[2, 1]
FN <- conf_matrix[1, 2]
misclassification_rate <- (FP + FN) / sum(conf_matrix)
TPR <- TP / (TP + FN)  # Sensitivity
TNR <- TN / (TN + FP)  # Specificity
balanced_accuracy <- (TPR + TNR) / 2  # Balanced Accuracy
return(list(
MisclassificationRate = misclassification_rate,
BalancedAccuracy = balanced_accuracy
))
}
# Evaluate metrics for both models
metrics_undersample <- compute_metrics(conf_test_undersample)
metrics_oversample <- compute_metrics(conf_test_oversample)
# Print results
list(
UndersamplingMetrics = metrics_undersample,
OversamplingMetrics = metrics_oversample
)
# Train RDA model on undersampled data
rda_model_undersample <- rda(Status ~ ., data = balanced_train)
# Train RDA model on undersampled data
rda_model_undersample <- rda(Status ~ ., data = balanced_train)
install.packages('klaR')
library(klaR)
# Train RDA model on undersampled data
rda_model_undersample <- rda(Status ~ ., data = balanced_train)
# Train RDA model on oversampled data
rda_model_oversample <- rda(Status ~ ., data = balanced_train2)
# Predictions on test set for both models
test_pred_rda_undersample <- predict(rda_model_undersample, newdata = test_data)$class
test_pred_rda_oversample <- predict(rda_model_oversample, newdata = test_data)$class
# Calculate confusion matrices for test set
conf_test_rda_undersample <- table(Predicted = test_pred_rda_undersample, Actual = test_data$Status)
conf_test_rda_oversample <- table(Predicted = test_pred_rda_oversample, Actual = test_data$Status)
# Compute evaluation metrics for both models
metrics_rda_undersample <- compute_metrics(conf_test_rda_undersample)
metrics_rda_oversample <- compute_metrics(conf_test_rda_oversample)
# Print results
list(
UndersamplingMetricsRDA = metrics_rda_undersample,
OversamplingMetricsRDA = metrics_rda_oversample
)
View(oversampled_1)
rda_model$regularization
# Gamma and Lambda for undersampling model
undersample_regularization <- rda_model_undersample$regularization
print(undersample_regularization)
# Gamma and Lambda for oversampling model
oversample_regularization <- rda_model_oversample$regularization
print(oversample_regularization)
