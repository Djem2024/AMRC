# Make predictions using the optimal Ridge model
predictions_ridge <- predict(cv_ridge_model, newx = X_test, s = "lambda.min")
# Plot the predicted vs actual values
plot(y_test, predictions_ridge, main = "Ridge Regression: Predicted vs Actual",
xlab = "Actual Values", ylab = "Predicted Values", pch = 19, col = "blue")
abline(a = 0, b = 1, col = "red")  # Add a 45-degree line for comparison
# Calculate RMSE (Root Mean Squared Error)
rmse_ridge <- sqrt(mean((predictions_ridge - y_test)^2))
cat("RMSE for Ridge Regression: ", rmse_ridge, "\n")
# Build the Lasso model with alpha = 1 (Lasso regression)
lasso_model <- glmnet(X_train, y_train, alpha = 1)
# Plot the model coefficients against log(lambda)
plot(lasso_model)
# Perform cross-validation to select the optimal lambda for Lasso
cv_lasso_model <- cv.glmnet(X_train, y_train, alpha = 1)
# Plot the results of the cross-validation
plot(cv_lasso_model)
# Get the optimal lambda value for Lasso regression
optimal_lambda_lasso <- cv_lasso_model$lambda.min
cat("Optimal lambda for Lasso: ", optimal_lambda_lasso, "\n")
# Make predictions on the test data using the optimal Lasso model
predictions_lasso <- predict(cv_lasso_model, newx = X_test, s = "lambda.min")
# Plot the predicted vs actual values for Lasso regression
plot(y_test, predictions_lasso, main = "Lasso Regression: Predicted vs Actual",
xlab = "Actual Values", ylab = "Predicted Values", pch = 19, col = "green")
abline(a = 0, b = 1, col = "red")  # Add a 45-degree line for comparison
# Calculate the RMSE for the Lasso model
rmse_lasso <- sqrt(mean((predictions_lasso - y_test)^2))
cat("RMSE for Lasso Regression: ", rmse_lasso, "\n")
# Extract Ridge model coefficients
ridge_coefficients <- coef(cv_ridge_model, s = "lambda.min")
# Create weights by inverting the absolute values of coefficients (excluding zero values)
adaptive_weights <- 1 / abs(ridge_coefficients)
adaptive_weights[is.infinite(adaptive_weights)] <- 0  # Set infinite values to zero
# Check weights
print(adaptive_weights)
# Build Adaptive Lasso model
adaptive_lasso_model <- glmnet(X_train, y_train, alpha = 1, penalty.factor = adaptive_weights)
# Cross-validation for Adaptive Lasso
cv_adaptive_lasso_model <- cv.glmnet(X_train, y_train, alpha = 1, penalty.factor = adaptive_weights)
# Predictions on the test data using Adaptive Lasso
predictions_adaptive_lasso <- predict(cv_adaptive_lasso_model, newx = X_test, s = "lambda.min")
# Extract Ridge model coefficients, excluding the intercept
ridge_coefficients <- coef(cv_ridge_model, s = "lambda.min")[-1, ]  # Remove the intercept
# Create weights by inverting the absolute values of the coefficients
adaptive_weights <- 1 / abs(ridge_coefficients)
adaptive_weights[is.infinite(adaptive_weights)] <- 0  # Set infinite values to zero
# Check the length to ensure it matches the number of variables in X_train
print(length(adaptive_weights))
print(ncol(X_train))
# Build Adaptive Lasso model using the corrected weights
adaptive_lasso_model <- glmnet(X_train, y_train, alpha = 1, penalty.factor = adaptive_weights)
# Build Adaptive Lasso model using the corrected weights
adaptive_lasso_model <- glmnet(X_train, y_train, alpha = 1, penalty.factor = adaptive_weights)
# Cross-validation for Adaptive Lasso
cv_adaptive_lasso_model <- cv.glmnet(X_train, y_train, alpha = 1, penalty.factor = adaptive_weights)
# Visualize cross-validation results
plot(cv_adaptive_lasso_model)
# Get the optimal lambda value
optimal_lambda_adaptive <- cv_adaptive_lasso_model$lambda.min
cat("Optimal lambda for Adaptive Lasso: ", optimal_lambda_adaptive, "\n")
# Predictions on the test data using Adaptive Lasso
predictions_adaptive_lasso <- predict(cv_adaptive_lasso_model, newx = X_test, s = "lambda.min")
# Plot predicted vs actual values
plot(y_test, predictions_adaptive_lasso, main = "Adaptive Lasso Regression: Predicted vs Actual",
xlab = "Actual Values", ylab = "Predicted Values", pch = 19, col = "purple")
abline(a = 0, b = 1, col = "red")  # Add y=x line for comparison
# Calculate RMSE for Adaptive Lasso
rmse_adaptive_lasso <- sqrt(mean((predictions_adaptive_lasso - y_test)^2))
cat("RMSE for Adaptive Lasso Regression: ", rmse_adaptive_lasso, "\n")
# Build Adaptive Lasso model using the corrected weights
adaptive_lasso_model <- glmnet(X_train, y_train, alpha = 1, penalty.factor = adaptive_weights)
# Plot the Adaptive Lasso model
plot(adaptive_lasso_model, xvar = "lambda", label = TRUE)
title("Coefficient Paths for Adaptive Lasso Regression")
# Extract Ridge model coefficients, excluding the intercept
ridge_coefficients <- coef(cv_ridge_model, s = "lambda.min")[-1, ]  # Remove the intercept
# Create weights by inverting the absolute values of the coefficients
adaptive_weights <- 1 / abs(ridge_coefficients)
adaptive_weights[is.infinite(adaptive_weights)] <- 0  # Set infinite values to zero
# Check the length to ensure it matches the number of variables in X_train
print(length(adaptive_weights))
print(ncol(X_train))
# Cross-validation for Adaptive Lasso
cv_adaptive_lasso_model <- cv.glmnet(X_train, y_train, alpha = 1, penalty.factor = adaptive_weights)
# Visualize cross-validation results
plot(cv_adaptive_lasso_model)
# Get the optimal lambda value
optimal_lambda_adaptive <- cv_adaptive_lasso_model$lambda.min
cat("Optimal lambda for Adaptive Lasso: ", optimal_lambda_adaptive, "\n")
# Adaptive Lasso Coefficients
adaptive_lasso_coefficients <- coef(cv_adaptive_lasso_model, s = "lambda.min")
# Comparing Lasso and Adaptive Lasso coefficients
print("Coefficients for Lasso Regression:")
print(lasso_coefficients)
# Adaptive Lasso Coefficients
adaptive_lasso_coefficients <- coef(cv_adaptive_lasso_model, s = "lambda.min")
# Comparing Lasso and Adaptive Lasso coefficients
print("Coefficients for Lasso Regression:")
print(lasso_coefficients)
# Extract Lasso model coefficients (excluding the intercept)
lasso_coefficients <- coef(cv_lasso_model, s = "lambda.min")
# Adaptive Lasso Coefficients
adaptive_lasso_coefficients <- coef(cv_adaptive_lasso_model, s = "lambda.min")
# Comparing Lasso and Adaptive Lasso coefficients
print("Coefficients for Lasso Regression:")
print(lasso_coefficients)
print("Coefficients for Adaptive Lasso Regression:")
print(adaptive_lasso_coefficients)
# Extract Lasso model coefficients (excluding the intercept)
lasso_coefficients <- coef(cv_lasso_model, s = "lambda.min")
# Adaptive Lasso Coefficients
adaptive_lasso_coefficients <- coef(cv_adaptive_lasso_model, s = "lambda.min")
# Comparing Lasso and Adaptive Lasso coefficients
print("Coefficients for Lasso Regression:")
print(lasso_coefficients)
print("Coefficients for Adaptive Lasso Regression:")
print(adaptive_lasso_coefficients)
install.packages(ROCit)
install.packages('ROCit')
knitr::opts_chunk$set(echo = TRUE)
library(ROCit)
load('Loan')
library(ROCit)
data(Loan)
str(Loan)
library(ROCit)
df = data(Loan)
str(df)
library(ROCit)
data(Loan)
library(ROCit)
data(Loan)
str(Loan)
library(ROCit)
data(Loan)
str(Loan)
head(Loan)
library(ROCit)
data(Loan)
head(Loan)
# Convert Status to a numeric variable (0 and 1)
Loan$Status <- as.numeric(Loan$Status)
str(Loan)
library(ROCit)
data(Loan)
head(Loan)
# Convert Status to a numeric variable (0 and 1)
Loan$Status <- as.numeric(Loan$Status)
str(Loan)
head(Loan)
library(ROCit)
data(Loan)
# Convert Status to a numeric variable (0 and 1)
Loan$Status <- as.numeric(Loan$Status)
str(Loan)
head(Loan)
set.seed(1234)
train_indices <- sample(1:nrow(Loan), size = 2/3*nrow(Loan))
train_data <- Loan[train_indices, ]
test_data <- Loan[-train_indices, ]
# Split the data
set.seed(1234)
train_indices <- sample(1:nrow(Loan), size = 2/3*nrow(Loan))
train_data <- Loan[train_indices, ]
test_data <- Loan[-train_indices, ]
# Fit a linear regression model on the training data
lm_model <- lm(Status ~., data = train_data)
summary(lm_model)
library(ROCit)
data(Loan)
# Convert Status to a numeric variable (0 and 1)
Loan$Status <- as.numeric(Loan$Status)
str(Loan)
head(Loan)
is.null(Loan)
library(ROCit)
data(Loan)
# Convert Status to a numeric variable (0 and 1)
Loan$Status <- as.numeric(Loan$Status)
str(Loan)
head(Loan)
is.na(Loan)
library(ROCit)
data(Loan)
# Convert Status to a numeric variable (0 and 1)
Loan$Status <- as.numeric(Loan$Status)
str(Loan)
head(Loan)
is.na(Loan)
sum(is.na(Loan))
plot(lm_model)
train_predictions <- predict(lm_model, newdata = train_data)
plot(train_data$Status, train_predictions, main = "Predictions vs Observations", xlab = "Observations", ylab = "Predicted Values")
# Apply cutoff (e.g., 0.5)
predicted_class <- ifelse(train_predictions > 0.5, 1, 0)
# Confusion matrix
table(Actual = train_data$Status, Predicted = predicted_class)
# Apply cutoff (e.g., 0.5)
predicted_class <- ifelse(train_predictions > 0.7, 1, 0)
# Confusion matrix
table(Actual = train_data$Status, Predicted = predicted_class)
# Apply cutoff (e.g., 0.5)
predicted_class <- ifelse(train_predictions > 0.7, 1, 0)
# Confusion matrix
table(Actual = train_data$Status, Predicted = predicted_class)
# Apply cutoff (e.g., 0.5)
predicted_class <- ifelse(train_predictions > 0.7, 1, 0)
# Confusion matrix
table(Actual = train_data$Status, Predicted = predicted_class)
# Apply cutoff (e.g., 0.5)
predicted_class <- ifelse(train_predictions > 0.8, 1, 0)
# Confusion matrix
table(Actual = train_data$Status, Predicted = predicted_class)
# Apply cutoff (e.g., 0.5)
predicted_class <- ifelse(train_predictions > 0.8, 1, 0)
# Confusion matrix
table(Actual = train_data$Status, Predicted = predicted_class)
# Apply cutoff (e.g., 0.5)
predicted_class <- ifelse(train_predictions > 0.2, 1, 0)
# Confusion matrix
table(Actual = train_data$Status, Predicted = predicted_class)
# Apply cutoff (e.g., 0.5)
predicted_class <- ifelse(train_predictions > 0.2, 1, 0)
# Confusion matrix
table(Actual = train_data$Status, Predicted = predicted_class)
# Apply cutoff (e.g., 0.5)
predicted_class <- ifelse(train_predictions > 0.1, 1, 0)
# Confusion matrix
table(Actual = train_data$Status, Predicted = predicted_class)
# Apply cutoff (e.g., 0.5)
predicted_class <- ifelse(train_predictions > 0.1, 1, 0)
# Confusion matrix
table(Actual = train_data$Status, Predicted = predicted_class)
# Apply cutoff (e.g., 0.5)
predicted_class <- ifelse(train_predictions > 0.9, 1, 0)
# Confusion matrix
table(Actual = train_data$Status, Predicted = predicted_class)
# Apply cutoff (e.g., 0.5)
predicted_class <- ifelse(train_predictions > 0.9, 1, 0)
# Confusion matrix
table(Actual = train_data$Status, Predicted = predicted_class)
# Apply cutoff (e.g., 0.5)
predicted_class <- ifelse(train_predictions > 0.5, 1, 0)
# Confusion matrix
table(Actual = train_data$Status, Predicted = predicted_class)
# Apply cutoff (e.g., 0.5)
predicted_class <- ifelse(train_predictions > 0.5, 1, 2)
# Confusion matrix
table(Actual = train_data$Status, Predicted = predicted_class)
# Apply cutoff (e.g., 0.5)
predicted_class <- ifelse(train_predictions > 0.5, 1, 2)
# Confusion matrix
table(Actual = train_data$Status, Predicted = predicted_class)
# Apply cutoff (e.g., 0.5)
predicted_class <- ifelse(train_predictions > 0.5, 1, 0)
# Confusion matrix
table(Actual = train_data$Status, Predicted = predicted_class)
# Apply cutoff (e.g., 0.5)
predicted_class <- ifelse(train_predictions > 0.7, 1, 0)
# Confusion matrix
table(Actual = train_data$Status, Predicted = predicted_class)
# Apply cutoff (e.g., 0.5)
predicted_class <- ifelse(train_predictions > 0.7, 1, 2)
# Confusion matrix
table(Actual = train_data$Status, Predicted = predicted_class)
roc_result <- rocit(prediction = train_predictions, Actual = train_data$Status)
roc_result <- rocit(prediction = train_predictions, Actual = train_data$Status)
roc_result <- rocit(scores = train_predictions, labels = train_data$Status)
roc_result <- rocit(prediction = train_predictions, Actual = train_data$Status)
knitr::opts_chunk$set(echo = TRUE)
library(ROCit)
data(Loan)
# Convert Status to a numeric variable (0 and 1)
Loan$Status <- as.numeric(Loan$Status)
str(Loan)
head(Loan)
# Split the data
set.seed(1234)
train_indices <- sample(1:nrow(Loan), size = 2/3*nrow(Loan))
train_data <- Loan[train_indices, ]
test_data <- Loan[-train_indices, ]
# Fit a linear regression model on the training data
lm_model <- lm(Status ~., data = train_data)
summary(lm_model)
plot(lm_model)
train_predictions <- predict(lm_model, newdata = train_data)
plot(train_data$Status, train_predictions, main = "Predictions vs Observations", xlab = "Observations", ylab = "Predicted Values")
# Apply cutoff (e.g., 0.5)
predicted_class <- ifelse(train_predictions > 0.5, 1, 0)
# Confusion matrix
table(Actual = train_data$Status, Predicted = predicted_class)
roc_result <- rocit(prediction = train_predictions, Actual = train_data$Status)
roc_result <- rocit(prediction = train_predictions, Actual = train_data$Status)
roc_result <- rocit(prediction = predicted_class, Actual = train_data$Status)
roc_result <- rocit(scores = train_predictions, labels = train_data$Status)
?rocit
roc_result <- rocit(score = train_predictions, class = train_data$Status)
summary(roc_result)
plot(roc_result)
# Apply cutoff (e.g., 0.5)
predicted_class <- ifelse(train_predictions > 0.7, 1, 0)
# Confusion matrix
table(Actual = train_data$Status, Predicted = predicted_class)
roc_result <- rocit(score = train_predictions, class = train_data$Status)
summary(roc_result)
plot(roc_result)
roc_result <- rocit(score = train_predictions, class = train_data$Status)
summary(roc_result)
plot(roc_result)
balanced_accuracy <- measureit(prediction = train_predictions, Actual = train_data$Status, measure = c("TPR", "TNR"))
balanced_accuracy <- measureit(score = train_predictions, Actual = train_data$Status, measure = c("TPR", "TNR"))
balanced_accuracy <- measureit(score = train_predictions, class = train_data$Status, measure = c("TPR", "TNR"))
plot(balanced_accuracy$cutoff, balanced_accuracy$balanced_accuracy, type = "l", xlab = "Cutoff", ylab = "Balanced Accuracy")
balanced_accuracy <- measureit(score = train_predictions, class = train_data$Status, measure = c("TPR", "TNR"))
plot(balanced_accuracy$cutoff, balanced_accuracy$balanced_accuracy, type = "l", xlab = "Cutoff", ylab = "Balanced Accuracy")
balanced_accuracy <- measureit(score = train_predictions, class = train_data$Status, measure = c("TPR", "TNR"))
balanced_accuracy <- measureit(score = train_predictions, class = train_data$Status, measure = c("TPR", "TNR"))
# Return the TPR and TNR values
return(c(TPR = measure_result$TPR, TNR = measure_result$TNR))
measure_result <- measureit(score = train_predictions, class = train_data$Status, measure = c("TPR", "TNR"))
# Return the TPR and TNR values
return(c(TPR = measure_result$TPR, TNR = measure_result$TNR))
measure_result <- measureit(score = train_predictions, class = train_data$Status, measure = c("TPR", "TNR"))
# Return the TPR and TNR values
return(c(TPR = measure_result$TPR, TNR = measure_result$TNR))
measure_result <- measureit(score = predicted_class, class = train_data$Status, measure = c("TPR", "TNR"))
# Return the TPR and TNR values
return(c(TPR = measure_result$TPR, TNR = measure_result$TNR))
measure_result <- measureit(score = predicted_class, class = train_data$Status, measure = c("TPR", "TNR"))
# Return the TPR and TNR values
return(c(TPR = measure_result$TPR, TNR = measure_result$TNR))
measure_result <- measureit(score = train_predictions, class = train_data$Status, measure = c("TPR", "TNR"))
# Return the TPR and TNR values
balanced_accuracies[i] <- mean(c(measure_result$TPR, measure_result$TNR))
# Step 2: Define a sequence of possible cutoff values
cutoffs <- seq(0, 1, by = 0.01)
# Step 3: Initialize a vector to store balanced accuracy for each cutoff
balanced_accuracies <- numeric(length(cutoffs))
# Step 4: Loop over each cutoff value
for (i in 1:length(cutoffs)) {
# Step 4a: Convert predicted probabilities to class predictions based on the cutoff
predicted_classes <- ifelse(train_predictions >= cutoffs[i], 1, 2)  # Assuming binary classification: 1 and 2
# Step 4b: Use the measureit function to calculate TPR and TNR
measure_result <- measureit(prediction = predicted_classes, Actual = train_data$Status, measure = c("TPR", "TNR"))
# Step 4c: Calculate the balanced accuracy (average of TPR and TNR)
balanced_accuracies[i] <- mean(c(measure_result$TPR, measure_result$TNR))
}
# Step 2: Define a sequence of possible cutoff values
cutoffs <- seq(0, 1, by = 0.01)
# Step 3: Initialize a vector to store balanced accuracy for each cutoff
balanced_accuracies <- numeric(length(cutoffs))
# Step 4: Loop over each cutoff value
for (i in 1:length(cutoffs)) {
# Step 4a: Convert predicted probabilities to class predictions based on the cutoff
predicted_classes <- ifelse(train_predictions >= cutoffs[i], 1, 2)  # Assuming binary classification: 1 and 2
# Step 4b: Use the measureit function to calculate TPR and TNR
measure_result <- measureit(score = predicted_classes, class = train_data$Status, measure = c("TPR", "TNR"))
# Step 4c: Calculate the balanced accuracy (average of TPR and TNR)
balanced_accuracies[i] <- mean(c(measure_result$TPR, measure_result$TNR))
}
# Step 5: Plot the Balanced Accuracy vs. Cutoff value
plot(cutoffs, balanced_accuracies, type = "l", col = "blue",
xlab = "Cutoff Value", ylab = "Balanced Accuracy", main = "Balanced Accuracy vs. Cutoff")
# Step 6: Identify the optimal cutoff (the one with the highest balanced accuracy)
optimal_cutoff <- cutoffs[which.max(balanced_accuracies)]
# Step 7: Print the optimal cutoff value
print(paste("Optimal cutoff:", optimal_cutoff))
measure_result <- measureit(score = predicted_class, class = train_data$Status, measure = c("TPR", "TNR"))
# Шаг 3: Вычисляем сбалансированную точность
balanced_accuracy <- mean(c(measure_result$TPR, measure_result$TNR))
# Выводим сбалансированную точность
print(paste("Сбалансированная точность для порога 0.5:", balanced_accuracy))
# Step 2: Define a sequence of possible cutoff values
cutoffs <- seq(0, 1, by = 0.01)
# Step 3: Initialize a vector to store balanced accuracy for each cutoff
balanced_accuracies <- numeric(length(cutoffs))
# Step 4: Loop over each cutoff value
for (i in 1:length(cutoffs)) {
# Step 4a: Convert predicted probabilities to class predictions based on the cutoff
predicted_classes <- ifelse(train_predictions >= cutoffs[i], 1, 2)  # Assuming binary classification: 1 and 2
# Step 4b: Use the measureit function to calculate TPR and TNR
measure_result <- measureit(score = predicted_classes, class = train_data$Status, measure = c("TPR", "TNR"))
# Step 4c: Calculate the balanced accuracy (average of TPR and TNR)
balanced_accuracies[i] <- mean(c(measure_result$TPR, measure_result$TNR))
}
# Step 5: Plot the Balanced Accuracy vs. Cutoff value
plot(cutoffs, balanced_accuracies, type = "l", col = "blue",
xlab = "Cutoff Value", ylab = "Balanced Accuracy", main = "Balanced Accuracy vs. Cutoff")
# Step 6: Identify the optimal cutoff (the one with the highest balanced accuracy)
optimal_cutoff <- cutoffs[which.max(balanced_accuracies)]
# Step 7: Print the optimal cutoff value
print(paste("Optimal cutoff:", optimal_cutoff))
knitr::opts_chunk$set(echo = TRUE)
# Initialize vectors
cutoff_values <- seq(0, 1, by = 0.05)
balanced_accuracies <- numeric(length(cutoff_values))
# Loop through cutoff values and calculate balanced accuracy
for (i in seq_along(cutoff_values)) {
predicted_class <- ifelse(train_predictions_prob > cutoff_values[i], 1, 0)
measure_result <- measureit(score = predicted_class, class = train_data$Status, measure = c("TPR", "TNR"))
balanced_accuracies[i] <- mean(c(measure_result$TPR, measure_result$TNR))
}
# Initialize vectors
cutoff_values <- seq(0, 1, by = 0.05)
balanced_accuracies <- numeric(length(cutoff_values))
# Loop through cutoff values and calculate balanced accuracy
for (i in seq_along(cutoff_values)) {
predicted_class <- ifelse(train_predictions > cutoff_values[i], 1, 0)
measure_result <- measureit(score = predicted_class, class = train_data$Status, measure = c("TPR", "TNR"))
balanced_accuracies[i] <- mean(c(measure_result$TPR, measure_result$TNR))
}
knitr::opts_chunk$set(echo = TRUE)
library(ROCit)
data(Loan)
# Convert Status to a numeric variable (0 and 1)
Loan$Status <- as.numeric(Loan$Status)
str(Loan)
head(Loan)
# Split the data
set.seed(1234)
train_indices <- sample(1:nrow(Loan), size = 2/3*nrow(Loan))
train_data <- Loan[train_indices, ]
test_data <- Loan[-train_indices, ]
# Fit a linear regression model on the training data
lm_model <- lm(Status ~., data = train_data)
summary(lm_model)
plot(lm_model)
train_predictions <- predict(lm_model, newdata = train_data)
plot(train_data$Status, train_predictions, main = "Predictions vs Observations", xlab = "Observations", ylab = "Predicted Values")
# Apply cutoff (e.g., 0.5)
predicted_class <- ifelse(train_predictions > 0.5, 1, 0)
# Confusion matrix
table(Actual = train_data$Status, Predicted = predicted_class)
roc_result <- rocit(score = train_predictions, class = train_data$Status)
summary(roc_result)
plot(roc_result)
# Initialize vectors
cutoff_values <- seq(0, 1, by = 0.05)
balanced_accuracies <- numeric(length(cutoff_values))
# Loop through cutoff values and calculate balanced accuracy
for (i in seq_along(cutoff_values)) {
predicted_class <- ifelse(train_predictions > cutoff_values[i], 1, 0)
measure_result <- measureit(score = predicted_class, class = train_data$Status, measure = c("TPR", "TNR"))
balanced_accuracies[i] <- mean(c(measure_result$TPR, measure_result$TNR))
}
# Plot balanced accuracy vs cutoff
plot(cutoff_values, balanced_accuracies, type = "b",
xlab = "Cutoff Value", ylab = "Balanced Accuracy",
main = "Balanced Accuracy vs Cutoff Value")
# Find optimal cutoff (max balanced accuracy)
optimal_cutoff <- cutoff_values[which.max(balanced_accuracies)]
abline(v = optimal_cutoff, col = "red", lty = 2)  # Add vertical line at optimal cutoff
# Print optimal cutoff and balanced accuracy
cat("Optimal cutoff:", optimal_cutoff, "\n")
cat("Balanced accuracy at optimal cutoff:", max(balanced_accuracies), "\n")
library(ROCit)
data(Loan)
# Convert Status to a numeric variable (0 and 1)
Loan$Status <- as.numeric(Loan$Status) - 1
str(Loan)
head(Loan)
# Split the data
set.seed(1234)
train_indices <- sample(1:nrow(Loan), size = 2/3*nrow(Loan))
train_data <- Loan[train_indices, ]
test_data <- Loan[-train_indices, ]
# Fit a linear regression model on the training data
lm_model <- lm(Status ~., data = train_data)
summary(lm_model)
plot(lm_model)
train_predictions <- predict(lm_model, newdata = train_data)
plot(train_data$Status, train_predictions, main = "Predictions vs Observations", xlab = "Observations", ylab = "Predicted Values")
# Apply cutoff (e.g., 0.5)
predicted_class <- ifelse(train_predictions > 0.5, 1, 0)
# Confusion matrix
table(Actual = train_data$Status, Predicted = predicted_class)
roc_result <- rocit(score = train_predictions, class = train_data$Status)
summary(roc_result)
plot(roc_result)
roc_result <- rocit(score = train_predictions, class = train_data$Status)
summary(roc_result)
plot(roc_result)
# Initialize vectors
cutoff_values <- seq(0, 1, by = 0.05)
balanced_accuracies <- numeric(length(cutoff_values))
# Loop through cutoff values and calculate balanced accuracy
for (i in seq_along(cutoff_values)) {
predicted_class <- ifelse(train_predictions > cutoff_values[i], 1, 0)
measure_result <- measureit(score = predicted_class, class = train_data$Status, measure = c("TPR", "TNR"))
balanced_accuracies[i] <- mean(c(measure_result$TPR, measure_result$TNR))
}
# Plot balanced accuracy vs cutoff
plot(cutoff_values, balanced_accuracies, type = "b",
xlab = "Cutoff Value", ylab = "Balanced Accuracy",
main = "Balanced Accuracy vs Cutoff Value")
# Find optimal cutoff (max balanced accuracy)
optimal_cutoff <- cutoff_values[which.max(balanced_accuracies)]
abline(v = optimal_cutoff, col = "red", lty = 2)  # Add vertical line at optimal cutoff
# Print optimal cutoff and balanced accuracy
cat("Optimal cutoff:", optimal_cutoff, "\n")
cat("Balanced accuracy at optimal cutoff:", max(balanced_accuracies), "\n")
# Predict on the test set using the optimal cutoff
test_predictions_prob <- predict(lm_model, newdata = test_data, type = "response")
test_predicted_class <- ifelse(test_predictions_prob > optimal_cutoff, 1, 0)
# Confusion matrix for test set
test_confusion_matrix <- table(Actual = test_data$Status, Predicted = test_predicted_class)
print(test_confusion_matrix)
# Apply cutoff (e.g., 0.5)
predicted_class <- ifelse(train_predictions > 0.5, 1, 0)
# Confusion matrix
table(Actual = train_data$Status, Predicted = predicted_class)
